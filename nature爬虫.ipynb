{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, time, os, json, random\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_info(soup):\n",
    "    ul = soup.find(\"ul\", {\"class\":\"app-article-list-row\"})\n",
    "    li_ls = ul.find_all(\"li\", {\"class\":\"app-article-list-row__item\"})\n",
    "    for li in li_ls:\n",
    "        a = li.find(\"a\", {\"class\":\"c-card__link u-link-inherit\"})\n",
    "        href = \"https://www.nature.com\" + a.attrs[\"href\"]\n",
    "        with open(\"./nature/url_ls.txt\", \"at\", encoding = 'utf-8') as fo:\n",
    "            fo.write(href)\n",
    "            fo.write('\\n')\n",
    "    pass\n",
    "\n",
    "\n",
    "def main(input_string):\n",
    "    url = \"https://www.nature.com/search?q={}\".format('+'.join(input_string.split()))\n",
    "    header = {\n",
    "        \"authority\":\"www.nature.com\",\n",
    "        \"user-agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.54 Safari/537.36\"\n",
    "    }\n",
    "    r = requests.get(url, headers = header)\n",
    "    soup = bs(r.text, \"html.parser\")\n",
    "    span = soup.find(\"div\", {\"class\":\"c-list-header\"}).find(\"span\", {\"class\":\"u-display-flex\"})\n",
    "    num_of_thesis = int(span.find_all(\"span\")[-1].string.split()[0])   # 这个是文献数量\n",
    "    get_page_info(soup)   # 直接传入soup获取搜索页面信息\n",
    "    page = num_of_thesis//50 + 1\n",
    "    print(\"\\r已完成 1/{}\".format(page), end = '')\n",
    "    if num_of_thesis <= 50:\n",
    "        return 0\n",
    "    for i in range(2, page + 1):\n",
    "        time.sleep(2 + random.random())\n",
    "        url_i = \"https://www.nature.com/search?q={}&page={}\".format('+'.join(input_string.split()), i)\n",
    "        r = requests.get(url_i, headers = header)\n",
    "        soup = bs(r.text, \"html.parser\")\n",
    "        get_page_info(soup)\n",
    "        print(\"\\r已完成 {}/{}\".format(i, page), end = '')\n",
    "    print('\\r获取搜索页面信息已完成')\n",
    "    pass\n",
    "\n",
    "\n",
    "input_string = input(\"请输入想搜索的内容：\")\n",
    "# input_string = \"Alkane activation\"\n",
    "start = time.time()\n",
    "main(input_string)\n",
    "print(\"耗时 {}s\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_soup(soup, url):\n",
    "    article = soup.find(\"article\")\n",
    "    try:\n",
    "        title = article.find('h1').string.strip()\n",
    "    except:\n",
    "        title = str(article.find('h1'))\n",
    "        replace_ls = [\"<sub>\", \"</sub>\", \"<sup>\", \"</sup>\", \"<i>\", \"</i>\", \"\\n\"]\n",
    "        for replace_str in replace_ls:\n",
    "            title = title.replace(replace_str, '')\n",
    "        title = title.split('<')[-2].split('>')[-1]\n",
    "    article_dic = {}\n",
    "    article_dic[title] = {}\n",
    "    sections = article.find_all(\"section\")\n",
    "    for section in sections:\n",
    "        try:\n",
    "            data_title = section.attrs[\"data-title\"]\n",
    "        except:\n",
    "            continue\n",
    "        article_dic[title][data_title] = []\n",
    "        p_ls = section.find_all('p')\n",
    "        for p in p_ls:\n",
    "            p_1 = [sup.extract() for sup in soup(\"sup\")]    # 去除p标签中的sup标签\n",
    "            p_2 = [sup.extract() for sup in soup(\"sub\")]    # 去除p标签中的sub标签\n",
    "            article_dic[title][data_title].append(str(p).strip('<p>').strip('</p>'))\n",
    "            # 这一句是从p标签中提取文本\n",
    "    with open(\"./nature/thesis.json\", \"at\", encoding = \"utf-8\") as fo:\n",
    "        # 所有文献保存在同一个json文件中\n",
    "        fo.write(json.dumps(article_dic))\n",
    "        fo.write('\\n')\n",
    "        \n",
    "\n",
    "\n",
    "def get_thesis():\n",
    "    with open(\"./nature/url_ls.txt\", \"rt\", encoding = 'utf-8') as fi:\n",
    "        for i in fi:\n",
    "            url = i.strip()\n",
    "            header = {\n",
    "                \"authority\":\"www.nature.com\",\n",
    "                \"user-agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.54 Safari/537.36\"\n",
    "            }\n",
    "            r = requests.get(url, headers = header)\n",
    "            r.encoding = r.apparent_encoding\n",
    "            soup = bs(r.text, \"html.parser\")\n",
    "            process_soup(soup, url)\n",
    "            time.sleep(2 + random.random())\n",
    "    pass\n",
    "            \n",
    "\n",
    "start_2 = time.time()\n",
    "get_thesis()\n",
    "print(\"文献爬取完成\")\n",
    "print(\"耗时 {}s\".format(time.time() - start_2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
